# Audio-visual Retrieval Challenge-21

## Problem Description
Given a query example in one modality (audio/video) the task is to retrieve relevent examples in other modality (video/audio). For every data point, audio and video modality is available along with the class level annotation. The class name can also be considered as the third modality text. The retrieval examples are correct if they are semantically similar to query, i.e. they share same class label as the query.

## Dataset Statistics
[AudiosetZSL](https://github.com/krantiparida/AudioSetZSL) dataset will be used for the task. This dataset is proposed for the task of zero-shot classification and retrieval of videos and is curated from a large dataset, [Audioset](http://research.google.com/audioset/). For this challenge, only the **seen** classes from the dataset will be considered. It contains a total of 79795 training examples and 26587 validation example. Out of the total 26593 testing examples, a subset of it will be used for the final evaluation. We have provided the features for both audio and video, extracted using pre-trained networks. For a fair comparison of the approach it is mandatory for everyone to use the features provided. More details about the dataset and task can be found in these papers below.
1. [Coordinated Joint Multimodal Embeddings](https://arxiv.org/pdf/1910.08732.pdf) 
2. [Discriminative Semantic Transitive Consistency](https://arxiv.org/abs/2103.14103)

## Evaluation Metric
ClassAverage mAP will be used as the evaluation metric. Each retrieval example will produce an average precision (AP) score. Averageing AP for all the query from a particular class will give the mAP for that class. ClassAverage mAP is then obtained by averaging mAP for all the class. ClassAverage Map can be calculated for both audio to video and video to audio retrieval. The final score will be the average of both of them.

```math
Final mAP = 0.5*(audio2video) + 0.5*(video2audio)
```

## Code to get started
### Data Download
1. Download the dataset from this [link](https://drive.google.com/drive/folders/1C5c8K0ZkPhzz-RX1qkPgkGejAssSA72v) onto ```data``` folder.
2. Arrange as per the directory structure given in the ```readme.md``` file inside ```data``` folder.
### Run Baselines (Unsupervised)
Different baseline codes using unsupervised approach are provided to start with.
1. Run ```python main_baseline.py``` for obtaining the baseline retrieval resutls from raw features directly.
2. Run ```python main_baseline.py -mode cca``` for obtaining the results using CCA.
3. Run ```python main_baseline.py -mode gcca``` for obtaining the results using GCCA.
### Run Baselines (Supervised)
A supervised learning baseline using triplet loss is also provided.
1. Run ```python main_triplet.py``` to learn a neural network model for aligning all modalities using a triplet loss.
2. Run ```python evaluate_triplet.py``` to evaluate using the model learnt in the previous step.

The code is tested with
```
python3.8
torch==1.9.0
numpy==1.21.2
scipy==1.7.1
h5py==3.4.0
pandas===1.3.2
```

## Submission
Submit a txt file with each row specifying the class label index of the retrieval results sorted in decreasing order of similarity. 
The first element in each row should also contain the class label index of the query example.
Two separate txt file should be submitted for audio to video and video to audio retrieval respectively.
Please note that the text file required for submission can be generated by specifying ```out_txt=True```in ```calculate_both_map``` function. 


